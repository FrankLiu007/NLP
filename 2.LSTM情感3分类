#%%

%matplotlib inline
import pandas as pd
import matplotlib
import numpy as np
import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['SimHei']  
plt.rcParams['axes.unicode_minus'] = False 
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D
from sklearn.model_selection import train_test_split
from keras.utils.np_utils import to_categorical
from keras.callbacks import EarlyStopping
from keras.layers import Dropout
import jieba as jb
import re

#%%

# 一、载入数据
data=pd.read_csv("nCoV_100k_train.labled.csv",encoding='UTF-8')

#%%

# 二、数据清洗
# (1)去掉多余列,并改名
data.drop(["微博id","发布人账号","微博图片","微博视频","微博发布时间"],axis=1,inplace=True)
data.rename(columns={ '微博中文内容': 'review','情感倾向':'label'},inplace=True)
data.head(10)

#%%

# (2)删除缺失值和重复值
data.dropna(subset=['review','label'],inplace=True)  # 删除缺失值
data.drop_duplicates(inplace=True) # 删除重复值

#%%

data.drop(index=(data.loc[(data['label']=='-2')].index),inplace=True)
data.drop(index=(data.loc[(data['label']=='9')].index),inplace=True)
data.drop(index=(data.loc[(data['label']=='10')].index),inplace=True)
data.drop(index=(data.loc[(data['label']=='-')].index),inplace=True)
data.drop(index=(data.loc[(data['label']=='·')].index),inplace=True)
data.drop(index=(data.loc[(data['label']=='4')].index),inplace=True)

#%%

# 类目
d = {'label':data['label'].value_counts().index, 'count': data['label'].value_counts()}
data_label = pd.DataFrame(data=d).reset_index(drop=True)
data_label

#%%

# 转id
data['label_id'] = data['label'].factorize()[0]  # 映射
label_id_df = data[['label', 'label_id']].drop_duplicates().sort_values('label_id').reset_index(drop=True)
label_to_id = dict(label_id_df.values)
data.sample(5)

#%%

# 绘图查看类目'
data_label.plot(x='label', y='count', kind='bar', legend=False,  figsize=(8, 5))
plt.title("类目分布")
plt.ylabel('数量', fontsize=18)
plt.xlabel('类目', fontsize=18)

#%%

# 数据清洗(删除除了字母,数字,汉字以外的所有符号)
def remove_punctuation(line):
    line = str(line)
    if line.strip()=='':
        return ''
    rule = re.compile(u"[^a-zA-Z0-9\u4E00-\u9FA5]")
    line = rule.sub('',line)
    return line
# 加载停用词
def stopwordslist(filepath):  
    stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]  
    return stopwords  
stopwords = stopwordslist("chinesestopwords.txt")

#%%

# 调用清洗数据函数并查看
data['clean_review'] = data['review'].apply(remove_punctuation)
data.sample(10)

# 分词
data['cut_review'] = data['clean_review'].apply(lambda x: " ".join([w for w in list(jb.cut(x)) if w not in stopwords]))
data.head()


#%%

# 设置最频繁使用的50000个词(在texts_to_matrix是会取前MAX_NB_WORDS,会取前MAX_NB_WORDS列)
MAX_NB_WORDS = 50000
# 每条cut_review最大的长度
MAX_SEQUENCE_LENGTH = 250
# 设置Embeddingceng层的维度
EMBEDDING_DIM = 100

tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~', lower=True)
tokenizer.fit_on_texts(data['cut_review'].values)
word_index = tokenizer.word_index
print('共有 %s 个不相同的词语.' % len(word_index))

#%%

# LSTM建模
# 模型的第一次是嵌入层(Embedding)，它使用长度为100的向量来表示每一个词语
# SpatialDropout1D层在训练中每次更新时， 将输入单元的按比率随机设置为 0， 这有助于防止过拟合
# LSTM层包含100个记忆单元
# 输出层为包含10个分类的全连接层
# 由于是多分类，所以激活函数设置为'softmax'
# 由于是多分类, 所以损失函数为分类交叉熵categorical_crossentropy

#%%

X = tokenizer.texts_to_sequences(data['cut_review'].values)
#填充X,让X的各个列的长度统一
X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)

#多类标签的onehot展开
Y = pd.get_dummies(data['label_id']).values

print(X.shape)
print(Y.shape)

#%%

# 拆分训练集
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)

#%%

# LSTM建模
model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(3, activation='softmax')) 
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

#%%#%%

# 训练数据
epochs = 20
batch_size = 2048

history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,
                   callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])

#%%

accr = model.evaluate(X_test,Y_test)
print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))

#%%

# 绘图
plt.title('Loss')
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show()

#%%

# 绘图
plt.title('Accuracy')
plt.plot(history.history['acc'], label='train')
plt.plot(history.history['val_acc'], label='test')
plt.legend()
plt.show()

#%%

# 模型评估


y_pred = model.predict(X_test)
y_pred = y_pred.argmax(axis = 1)
Y_test = Y_test.argmax(axis = 1)

#%%

import seaborn as sns
from sklearn.metrics import accuracy_score, confusion_matrix
#生成混淆矩阵
conf_mat = confusion_matrix(Y_test, y_pred)
fig, ax = plt.subplots(figsize=(10,8))
sns.heatmap(conf_mat, annot=True, fmt='d',
            xticklabels=label_id_df.label.values, yticklabels=label_id_df.label.values)
plt.ylabel('实际结果',fontsize=18)
plt.xlabel('预测结果',fontsize=18)

#%%

from  sklearn.metrics import classification_report
 
print('accuracy %s' % accuracy_score(y_pred, Y_test))
print(classification_report(Y_test, y_pred,target_names=label_id_df['label'].values))

#%%

# 与上面代码无关,此为验证测试集调用模型的代码
# 保存
print("Saving model to disk \n")
print('test before save:',model.predict(X_test[0:2]))
mp = "E://学习代码/情感分析/iris_model.h5"
model.save(mp)

#%%

# 读取
from keras.models import load_model
model = load_model('iris_model.h5')
print('test before save:',model.predict(X_test[0:2]))   
